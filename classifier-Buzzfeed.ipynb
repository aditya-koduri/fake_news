{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "my_stem = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "dictionary = set(w.lower() for w in nltk.corpus.words.words())\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\adity\\Desktop\\Columbia\\Spring 2021\\DS_PP\\Assignment\\buzzfeed.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove articles with no text\n",
    "df = df[~df.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.4, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>fake</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>BuzzFeed_Real_71-Webpage.json</td>\n",
       "      <td>http://cnn.it</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Bridgegate: Port Authority officials planned t...</td>\n",
       "      <td>Newark (CNN) Former Port Authority executives ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BuzzFeed_Fake_16-Webpage.json</td>\n",
       "      <td>http://100percentfedup.com</td>\n",
       "      <td>2016-09-19</td>\n",
       "      <td>WHOA! NEW DISTURBING VIDEO Shows HILLARY'S Cam...</td>\n",
       "      <td>On September 15, Hillary â€œapparentlyâ€ held...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-19 19:39:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>BuzzFeed_Real_54-Webpage.json</td>\n",
       "      <td>http://www.ifyouonlynews.com</td>\n",
       "      <td>2016-09-25</td>\n",
       "      <td>Debate Commission STUNS Journalists By Siding ...</td>\n",
       "      <td>\\n\\n16505 SHARES SHARES FacebookTwitter Google...</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-25 10:21:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>BuzzFeed_Real_67-Webpage.json</td>\n",
       "      <td>http://abcn.ws</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Terrorist Attacks Likely to Affect 2016 Race, ...</td>\n",
       "      <td>The recent connected bombings in New York and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>BuzzFeed_Fake_73-Webpage.json</td>\n",
       "      <td>http://rightwingnews.com</td>\n",
       "      <td>2016-09-20</td>\n",
       "      <td>Michelle Obama NOT Leaving The White House - H...</td>\n",
       "      <td>Michelle Obama NOT Leaving The White House â€“...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-20 15:41:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ID                        source       date  \\\n",
       "159  BuzzFeed_Real_71-Webpage.json                 http://cnn.it        NaT   \n",
       "7    BuzzFeed_Fake_16-Webpage.json    http://100percentfedup.com 2016-09-19   \n",
       "140  BuzzFeed_Real_54-Webpage.json  http://www.ifyouonlynews.com 2016-09-25   \n",
       "154  BuzzFeed_Real_67-Webpage.json                http://abcn.ws        NaT   \n",
       "70   BuzzFeed_Fake_73-Webpage.json      http://rightwingnews.com 2016-09-20   \n",
       "\n",
       "                                                 title  \\\n",
       "159  Bridgegate: Port Authority officials planned t...   \n",
       "7    WHOA! NEW DISTURBING VIDEO Shows HILLARY'S Cam...   \n",
       "140  Debate Commission STUNS Journalists By Siding ...   \n",
       "154  Terrorist Attacks Likely to Affect 2016 Race, ...   \n",
       "70   Michelle Obama NOT Leaving The White House - H...   \n",
       "\n",
       "                                                  text  fake  \\\n",
       "159  Newark (CNN) Former Port Authority executives ...     0   \n",
       "7    On September 15, Hillary â€œapparentlyâ€ held...     1   \n",
       "140  \\n\\n16505 SHARES SHARES FacebookTwitter Google...     0   \n",
       "154  The recent connected bombings in New York and ...     0   \n",
       "70   Michelle Obama NOT Leaving The White House â€“...     1   \n",
       "\n",
       "              date_time  \n",
       "159                 NaT  \n",
       "7   2016-09-19 19:39:44  \n",
       "140 2016-09-25 10:21:37  \n",
       "154                 NaT  \n",
       "70  2016-09-20 15:41:33  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df = pd.DataFrame()\n",
    "## clean/prepare text\n",
    "for rev, outcome in zip(train.text.tolist(), train.fake.tolist()):\n",
    "    \n",
    "    # only keep words (remove other characters)\n",
    "    tmp_read = re.sub('[^a-zA-Z]+', ' ', rev).lower()\n",
    "\n",
    "    #Tokenization and remove stop words\n",
    "    tmp_read = [word for word in tmp_read.split() if word not in stop_words]\n",
    "\n",
    "    #dictionary words\n",
    "    dict_read = [word for word in tmp_read if word in dictionary]\n",
    "    \n",
    "    # stemming\n",
    "    tmp_read_stm = [my_stem.stem(word) for word in tmp_read]\n",
    "    dict_read_stm = [my_stem.stem(word) for word in dict_read]\n",
    "\n",
    "    # lemmatization\n",
    "    tmp_read_lem = [lemmatizer.lemmatize(word) for word in tmp_read]\n",
    "    dict_read_lem = [lemmatizer.lemmatize(word) for word in dict_read]\n",
    "\n",
    "    \n",
    "    # rejoin reviews\n",
    "    tmp_read = ' '.join(tmp_read)\n",
    "    tmp_read_stm = ' '.join(tmp_read_stm)\n",
    "    tmp_read_lem = ' '.join(tmp_read_lem)\n",
    "    \n",
    "    dict_read = ' '.join(dict_read)\n",
    "    dict_read_stm = ' '.join(dict_read_stm)\n",
    "    dict_read_lem = ' '.join(dict_read_lem)\n",
    "\n",
    "\n",
    "    # add to new df\n",
    "    tmp = pd.DataFrame([rev], columns=['original review'])\n",
    "    tmp['body'] = tmp_read\n",
    "    tmp['body_stem'] = tmp_read_stm\n",
    "    tmp['body_lem'] = tmp_read_lem\n",
    "    tmp['body_dict'] = dict_read\n",
    "    tmp['body_dict_stem'] = dict_read_stm\n",
    "    tmp['fake'] = outcome\n",
    "\n",
    "    the_df = the_df.append(tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original review</th>\n",
       "      <th>body</th>\n",
       "      <th>body_stem</th>\n",
       "      <th>body_lem</th>\n",
       "      <th>body_dict</th>\n",
       "      <th>body_dict_stem</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newark (CNN) Former Port Authority executives ...</td>\n",
       "      <td>newark cnn former port authority executives ne...</td>\n",
       "      <td>newark cnn former port author execut new jerse...</td>\n",
       "      <td>newark cnn former port authority executive new...</td>\n",
       "      <td>former port authority new jersey chris christi...</td>\n",
       "      <td>former port author new jersey chri christi mak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On September 15, Hillary â€œapparentlyâ€ held...</td>\n",
       "      <td>september hillary apparently held rally old st...</td>\n",
       "      <td>septemb hillari appar held ralli old student r...</td>\n",
       "      <td>september hillary apparently held rally old st...</td>\n",
       "      <td>september hillary apparently rally old student...</td>\n",
       "      <td>septemb hillari appar ralli old student recrea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n16505 SHARES SHARES FacebookTwitter Google...</td>\n",
       "      <td>shares shares facebooktwitter googlepinterestd...</td>\n",
       "      <td>share share facebooktwitt googlepinterestdiggl...</td>\n",
       "      <td>share share facebooktwitter googlepinterestdig...</td>\n",
       "      <td>former senator daniel patrick said everyone op...</td>\n",
       "      <td>former senat daniel patrick said everyon opini...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The recent connected bombings in New York and ...</td>\n",
       "      <td>recent connected bombings new york new jersey ...</td>\n",
       "      <td>recent connect bomb new york new jersey stab a...</td>\n",
       "      <td>recent connected bombing new york new jersey s...</td>\n",
       "      <td>recent connected new york new jersey stabbing ...</td>\n",
       "      <td>recent connect new york new jersey stab attack...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelle Obama NOT Leaving The White House â€“...</td>\n",
       "      <td>michelle obama leaving white house hillary cli...</td>\n",
       "      <td>michel obama leav white hous hillari clinton t...</td>\n",
       "      <td>michelle obama leaving white house hillary cli...</td>\n",
       "      <td>michelle leaving white house hillary clinton t...</td>\n",
       "      <td>michel leav white hous hillari clinton terrifi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original review  \\\n",
       "0  Newark (CNN) Former Port Authority executives ...   \n",
       "1  On September 15, Hillary â€œapparentlyâ€ held...   \n",
       "2  \\n\\n16505 SHARES SHARES FacebookTwitter Google...   \n",
       "3  The recent connected bombings in New York and ...   \n",
       "4  Michelle Obama NOT Leaving The White House â€“...   \n",
       "\n",
       "                                                body  \\\n",
       "0  newark cnn former port authority executives ne...   \n",
       "1  september hillary apparently held rally old st...   \n",
       "2  shares shares facebooktwitter googlepinterestd...   \n",
       "3  recent connected bombings new york new jersey ...   \n",
       "4  michelle obama leaving white house hillary cli...   \n",
       "\n",
       "                                           body_stem  \\\n",
       "0  newark cnn former port author execut new jerse...   \n",
       "1  septemb hillari appar held ralli old student r...   \n",
       "2  share share facebooktwitt googlepinterestdiggl...   \n",
       "3  recent connect bomb new york new jersey stab a...   \n",
       "4  michel obama leav white hous hillari clinton t...   \n",
       "\n",
       "                                            body_lem  \\\n",
       "0  newark cnn former port authority executive new...   \n",
       "1  september hillary apparently held rally old st...   \n",
       "2  share share facebooktwitter googlepinterestdig...   \n",
       "3  recent connected bombing new york new jersey s...   \n",
       "4  michelle obama leaving white house hillary cli...   \n",
       "\n",
       "                                           body_dict  \\\n",
       "0  former port authority new jersey chris christi...   \n",
       "1  september hillary apparently rally old student...   \n",
       "2  former senator daniel patrick said everyone op...   \n",
       "3  recent connected new york new jersey stabbing ...   \n",
       "4  michelle leaving white house hillary clinton t...   \n",
       "\n",
       "                                      body_dict_stem  fake  \n",
       "0  former port author new jersey chri christi mak...     0  \n",
       "1  septemb hillari appar ralli old student recrea...     1  \n",
       "2  former senat daniel patrick said everyon opini...     0  \n",
       "3  recent connect new york new jersey stab attack...     0  \n",
       "4  michel leav white hous hillari clinton terrifi...     1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(the_df['fake'])\n",
    "# choose column to use has main text for models\n",
    "body_text = np.array(the_df['body_lem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram vectorizer\n",
    "# vectorize\n",
    "my_vec_tfidf_out = TfidfVectorizer()\n",
    "my_xform_tfidf_out = my_vec_tfidf_out.fit_transform(body_text)\n",
    "my_pd = pd.DataFrame(my_xform_tfidf_out.toarray())\n",
    "my_pd.columns = my_vec_tfidf_out.get_feature_names()\n",
    "X = my_pd.values\n",
    "\n",
    "# vectorize test data\n",
    "test_vec=my_vec_tfidf_out.transform(test.text.tolist())\n",
    "my_pd = pd.DataFrame(test_vec.toarray())\n",
    "my_pd.columns = my_vec_tfidf_out.get_feature_names()\n",
    "X_test = my_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unigram/bigram vectorizer\n",
    "# my_vec_tfidf_out = TfidfVectorizer(ngram_range = (1,2)) \n",
    "# my_xform_tfidf_out = my_vec_tfidf_out.fit_transform(body_text)\n",
    "# my_pd = pd.DataFrame(my_xform_tfidf_out.toarray())\n",
    "# my_pd.columns = my_vec_tfidf_out.get_feature_names()\n",
    "# X = my_pd.values\n",
    "\n",
    "# test_vec=my_vec_tfidf_out.transform(test.text.tolist())\n",
    "# my_pd = pd.DataFrame(test_vec.toarray())\n",
    "# my_pd.columns = my_vec_tfidf_out.get_feature_names()\n",
    "# X_test = my_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   counts  percentage\n",
      "0      59    54.12844\n",
      "1      50    45.87156\n"
     ]
    }
   ],
   "source": [
    "# are the classes balanced?\n",
    "balance_check = pd.concat([the_df.fake.value_counts(), \n",
    "                the_df.fake.value_counts(normalize=True).mul(100)],axis=1, keys=('counts','percentage'))\n",
    "print(balance_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_recall_curve, plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "f1_scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.659\n",
      "test-set score: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "start = timeit.default_timer()\n",
    "logreg = LogisticRegression(penalty='none',random_state=9).fit(X_scaled, label)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "# # cross-validation for model evaluation\n",
    "cv_mean = np.mean(cross_val_score(LogisticRegression(penalty='none'), X_scaled, label, cv=5, scoring=f1_scorer))\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(cv_mean))\n",
    "\n",
    "y_predictions = logreg.predict(X_test_scaled)\n",
    "print(\"test-set score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))\n",
    "\n",
    "\n",
    "\n",
    "# print('model time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.686\n",
      "test-set score: 0.825\n",
      "model time:  1.125014399999941\n"
     ]
    }
   ],
   "source": [
    "# L1 Penalized Logistic Regression\n",
    "param_gridlasso = {'C': [0.001 , 0.01 , 0.1 , 1 , 10]} \n",
    "\n",
    "# cv = number of folds\n",
    "gridlasso = GridSearchCV(LogisticRegression(penalty='l1',solver='liblinear',random_state=9), \n",
    "                    param_grid=param_gridlasso, cv=5, scoring=f1_scorer)\n",
    "\n",
    "#use meta model methods to fit score and predict model:\n",
    "start = timeit.default_timer()\n",
    "gridlasso.fit(X_scaled, label)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(gridlasso.best_score_))\n",
    "y_predictions = gridlasso.predict(X_test_scaled)\n",
    "print(\"test-set score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))\n",
    "\n",
    "print('model time: ', stop - start) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 400}\n",
      "best mean cross-validation score: 0.621\n",
      "test-set score: 0.686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {'n_estimators': [400 , 500,600],\n",
    "             \"max_depth\":[3,4,5],\n",
    "             \"learning_rate\":[.001,.01,.1]} \n",
    "\n",
    "# cv = number of folds\n",
    "gridgb = GridSearchCV(GradientBoostingClassifier(random_state=9), \n",
    "                    param_grid=param_grid, cv=5, scoring=f1_scorer)\n",
    "\n",
    "#use meta model methods to fit score and predict model:\n",
    "start = timeit.default_timer()\n",
    "gridgb.fit(X, label)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"best parameters: {}\".format(gridgb.best_params_))\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(gridgb.best_score_))\n",
    "y_predictions = gridgb.predict(X_test)\n",
    "print(\"test-set score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-set score: 0.694\n",
      "test-set score: 0.699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=500, random_state=9)\n",
    "model.fit(X, label)\n",
    "y_predictions = model.predict(X_test)\n",
    "print(\"test-set score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))\n",
    "print(\"test-set score: {:.3f}\".format(accuracy_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'max_depth': 15, 'n_estimators': 500}\n",
      "best mean cross-validation score: 0.670\n",
      "test-set score: 0.625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {'n_estimators': [100 , 300,500,800],\n",
    "             \"max_depth\":[1,5,10,15]} \n",
    "\n",
    "# cv = number of folds\n",
    "gridrf = GridSearchCV(RandomForestClassifier(random_state=9), \n",
    "                    param_grid=param_grid, cv=5, scoring=f1_scorer)\n",
    "\n",
    "#use meta model methods to fit score and predict model:\n",
    "start = timeit.default_timer()\n",
    "gridrf.fit(X, label)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"best parameters: {}\".format(gridrf.best_params_))\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(gridrf.best_score_))\n",
    "\n",
    "y_predictions = gridrf.predict(X_test)\n",
    "print(\"test-set score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-set score: 0.914\n",
      "test-set score: 0.903\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, max_depth=15,random_state=9)\n",
    "model.fit(X, label)\n",
    "y_predictions = model.predict(X_test)\n",
    "print(\"test-set score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))\n",
    "print(\"test-set score: {:.3f}\".format(accuracy_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 5, 'gamma': 'auto', 'kernel': 'linear'}\n",
      "best mean cross-validation score: 0.827\n",
      "test-set f1-score: 0.878\n",
      "test-set acc score: 0.8611\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "param_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'gamma':['sclae','auto'],\n",
    "             \"C\":[1,5,8,10,15]} \n",
    "\n",
    "# cv = number of folds\n",
    "gridsvc = GridSearchCV(SVC(random_state=9), \n",
    "                    param_grid=param_grid, cv=5, scoring=f1_scorer)\n",
    "\n",
    "#use meta model methods to fit score and predict model:\n",
    "start = timeit.default_timer()\n",
    "gridsvc.fit(X, label)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print(\"best parameters: {}\".format(gridsvc.best_params_))\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(gridsvc.best_score_))\n",
    "\n",
    "y_predictions = gridsvc.predict(X_test)\n",
    "print(\"test-set f1-score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))\n",
    "print(\"test-set acc score: {:.4f}\".format(accuracy_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-set score: 0.8780\n",
      "test-set score: 0.8611\n"
     ]
    }
   ],
   "source": [
    "# body_lem -- unigram and bigram\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=5, gamma='auto', random_state=9).fit(X, label)\n",
    "# clf.score(X_test, np.array(test.fake))\n",
    "y_predictions = clf.predict(X_test)\n",
    "print(\"test-set score: {:.4f}\".format(f1_score(np.array(test.fake), y_predictions)))\n",
    "print(\"test-set score: {:.4f}\".format(accuracy_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:56:03] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:04] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:04] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:05] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:06] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:07] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:08] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:11] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:12] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:13] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:14] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:15] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:16] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:17] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:17] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:19] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:20] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:22] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:23] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:24] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:26] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:27] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:28] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:56:30] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:31] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:32] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:56:34] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best parameters: {'eta': 1, 'max_depth': 2, 'n_estimators': 100}\n",
      "best mean cross-validation score: 0.754\n",
      "test-set score: 0.805\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "param = {'max_depth':[1,2, 3], 'eta':[1,2], 'n_estimators':[100]}\n",
    "\n",
    "# cv = number of folds\n",
    "gridxgb = GridSearchCV(XGBClassifier(random_state=9), \n",
    "                    param_grid=param, cv=5, scoring=f1_scorer)\n",
    "#use meta model methods to fit score and predict model:\n",
    "start = timeit.default_timer()\n",
    "gridxgb.fit(X, label)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "y_predictions = gridxgb.predict(X_test)\n",
    "print(\"best parameters: {}\".format(gridxgb.best_params_))\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(gridxgb.best_score_))\n",
    "print(\"test-set score: {:.3f}\".format(f1_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:53] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "test-set score: 0.8052\n",
      "test-set score: 0.7917\n"
     ]
    }
   ],
   "source": [
    "xgboost = XGBClassifier(random_state=9, max_depth=2,eta=1,n_estimators=100)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "xgboost.fit(X, label)\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "# clf.score(X_test, np.array(test.fake))\n",
    "y_predictions = xgboost.predict(X_test)\n",
    "print(\"test-set score: {:.4f}\".format(f1_score(np.array(test.fake), y_predictions)))\n",
    "print(\"test-set score: {:.4f}\".format(accuracy_score(np.array(test.fake), y_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
